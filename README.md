# News Analyser - Advanced Financial News Sentiment Analysis Platform

A production-ready Django-based platform for real-time financial news sentiment analysis, specifically tailored for the Indian stock market. Combines multi-model AI sentiment analysis with comprehensive news scraping and intelligent ticker recognition.

## ğŸ¯ Overview

News Analyser is an institutional-grade financial news analysis system designed for Wall Street Club members at BITS Pilani. It provides real-time sentiment analysis of Indian financial news using multiple AI models including Google Gemini, FinBERT, VADER, and TextBlob, with sophisticated ticker recognition and sector-based analysis capabilities.

## âœ¨ Key Features

### Multi-Model Sentiment Analysis
- **Google Gemini AI**: Contextual financial sentiment analysis with market impact scoring
- **FinBERT**: Finance-specific BERT model for domain-aware sentiment detection
- **VADER**: Social sentiment analysis for rapid processing
- **TextBlob**: Baseline sentiment with polarity and subjectivity metrics
- **Ensemble Scoring**: Weighted combination of all models for maximum accuracy

### Comprehensive News Scraping
- **RSS Feed Scraping**: Automated collection from 6+ major Indian financial news sources
  - Economic Times
  - Business Standard
  - LiveMint
  - MoneyControl
  - The Hindu Business
  - Times of India Business
- **Web Scraping**: BeautifulSoup-based scraping for full article content
- **Selenium Support**: JavaScript-rendered content extraction
- **Smart Deduplication**: SimHash and content-based duplicate detection

### Intelligent Ticker Recognition
- **NSE Ticker Database**: Complete NSE India ticker list with fuzzy matching
- **Company Name Variations**: Recognizes abbreviations and common name variants
- **Confidence Scoring**: Similarity threshold-based matching (85%+)
- **Multi-Ticker Support**: Identifies all mentioned stocks in a single article

### Sector Analysis
- **10 Major Sectors**: Banking, IT, Pharma, Auto, FMCG, Energy, Metals, Telecom, Realty, Infrastructure
- **Sector Sentiment Trends**: Real-time aggregated sentiment by sector
- **Rotation Signals**: Identifies potential sector rotation based on sentiment shifts
- **Cross-Sector Impact**: Analyzes sentiment correlation between sectors

### REST API
- **Comprehensive Endpoints**: Full CRUD operations for news, sentiment, stocks, and sectors
- **Real-time Aggregations**: Ticker and sector sentiment calculations
- **Market Summary**: Daily market-wide sentiment analysis
- **Pagination & Filtering**: Advanced search, filtering, and ordering capabilities
- **Rate Limiting**: Built-in throttling for API protection

### Background Processing
- **Celery Task Queue**: Distributed processing for scraping and analysis
- **Scheduled Tasks**: Automatic scraping during market hours (9 AM - 4 PM IST)
- **Retry Logic**: Exponential backoff for failed operations
- **Parallel Processing**: Concurrent scraping and sentiment analysis

### Performance Optimizations
- **Redis Caching**: 5-minute cache for ticker sentiment, 15-minute for sector summaries
- **Database Indexing**: Optimized queries for sentiment scores and time-based lookups
- **Lazy Loading**: FinBERT model loaded on-demand
- **Connection Pooling**: Efficient database and Redis connections

## ğŸ—ï¸ Architecture

```
news-analyser/
â”œâ”€â”€ news_analyser/           # Main Django app
â”‚   â”œâ”€â”€ sentiment/           # Sentiment analysis module
â”‚   â”‚   â”œâ”€â”€ analyzer.py      # Multi-model sentiment analyzer
â”‚   â”‚   â”œâ”€â”€ sector_analyzer.py  # Sector-based analysis
â”‚   â”‚   â””â”€â”€ utils.py         # Ticker recognition & utilities
â”‚   â”œâ”€â”€ scraping/            # News scraping module
â”‚   â”‚   â”œâ”€â”€ base_scraper.py  # Abstract base scraper
â”‚   â”‚   â”œâ”€â”€ rss_scraper.py   # RSS feed scraper
â”‚   â”‚   â”œâ”€â”€ web_scraper.py   # BeautifulSoup scraper
â”‚   â”‚   â”œâ”€â”€ selenium_scraper.py  # Selenium scraper
â”‚   â”‚   â””â”€â”€ deduplication.py # Content deduplication
â”‚   â”œâ”€â”€ api/                 # REST API
â”‚   â”‚   â”œâ”€â”€ serializers.py   # DRF serializers
â”‚   â”‚   â”œâ”€â”€ views.py         # API viewsets
â”‚   â”‚   â””â”€â”€ urls.py          # API routing
â”‚   â”œâ”€â”€ models.py            # Database models
â”‚   â”œâ”€â”€ tasks.py             # Celery tasks
â”‚   â””â”€â”€ views.py             # Django views
â”œâ”€â”€ blackbox/                # Django project settings
â”œâ”€â”€ Ticker_List_NSE_India.csv  # NSE ticker database
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Redis 5.0+
- PostgreSQL 12+ (optional, uses SQLite by default)
- Chrome/Chromium (for Selenium scraping)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/rish-kun/news-analyser.git
cd news-analyser
```

2. **Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env and add your API keys
```

Required environment variables:
```env
GEMINI_API_KEY=your_gemini_api_key_here
SECRET_KEY=your_django_secret_key_here
CELERY_BROKER_URL=redis://localhost:6379/0
REDIS_URL=redis://localhost:6379/1
```

5. **Run migrations**
```bash
python manage.py makemigrations
python manage.py migrate
```

6. **Create superuser**
```bash
python manage.py createsuperuser
```

7. **Start Redis** (in separate terminal)
```bash
redis-server
```

8. **Start Celery worker** (in separate terminal)
```bash
celery -A blackbox worker -l info
```

9. **Start Celery beat** (in separate terminal)
```bash
celery -A blackbox beat -l info
```

10. **Run development server**
```bash
python manage.py runserver
```

Visit `http://localhost:8000` to access the application.

## ğŸ“Š Database Models

### News
- **Fields**: title, content, link, author, image_url, tags, published_at, is_analyzed
- **Relationships**: source, tickers, sectors, sentiment_scores
- **Indexes**: published_at, is_analyzed, content_hash

### SentimentScore
- **Fields**: gemini_score, finbert_score, vader_score, textblob_score, sentiment_score, sentiment_label, confidence
- **Relationships**: news, ticker, sector
- **Indexes**: ticker+created_at, sector+created_at, sentiment_score

### Stock
- **Fields**: name, symbol, sector
- **Relationships**: sentiment_scores, news_mentions

### Sector
- **Fields**: name, search_fields
- **Relationships**: stocks, sentiment_scores, news_mentions

## ğŸ”Œ API Endpoints

### News Endpoints
- `GET /api/news/` - List all news articles (paginated)
- `GET /api/news/{id}/` - Get specific article
- `GET /api/news/recent/?hours=24` - Get recent news
- `GET /api/news/search/?ticker=RELIANCE` - Search by ticker
- `GET /api/news/search/?q=keyword` - Search by keyword
- `POST /api/news/{id}/analyze/` - Trigger sentiment analysis
- `GET /api/news/by_sentiment/?min=0.5&max=1.0` - Filter by sentiment

### Sentiment Endpoints
- `GET /api/sentiment/ticker/{symbol}/` - Get ticker sentiment
- `GET /api/sentiment/sector/{name}/` - Get sector sentiment
- `GET /api/sentiment/market_summary/` - Get market summary
- `GET /api/sentiment/trending/` - Get trending sectors
- `GET /api/sentiment/rotation_signals/` - Get rotation signals

### Stock Endpoints
- `GET /api/stocks/` - List all stocks
- `GET /api/stocks/{id}/` - Get stock details
- `GET /api/stocks/{id}/sentiment_history/` - Get sentiment history
- `GET /api/stocks/{id}/recent_news/` - Get recent news

### Sector Endpoints
- `GET /api/sectors/` - List all sectors
- `GET /api/sectors/{id}/stocks/` - Get sector stocks
- `GET /api/sectors/{id}/sentiment/` - Get sector sentiment

## âš™ï¸ Celery Tasks

### Scraping Tasks
- `scrape_all_sources()` - Master scraping coordinator
- `scrape_rss_source(source_key)` - Scrape specific RSS source

### Analysis Tasks
- `analyze_article_sentiment(news_id)` - Analyze single article
- `analyze_pending_articles(limit)` - Batch analyze pending articles

### Aggregation Tasks
- `aggregate_ticker_sentiment(symbol, hours)` - Calculate ticker sentiment
- `aggregate_all_tickers_sentiment()` - Aggregate all active tickers

### Maintenance Tasks
- `cleanup_old_articles(days)` - Archive old articles
- `cleanup_cache()` - Clear expired cache entries

### Reporting Tasks
- `generate_market_summary()` - Generate daily market summary

## ğŸ“… Scheduled Tasks

| Task | Schedule | Description |
|------|----------|-------------|
| Scrape (market hours) | Every 30 min, 9AM-4PM IST | Scrape during trading hours |
| Scrape (off-hours) | Every 2 hours | Scrape outside trading hours |
| Analyze pending | Every hour | Process unanalyzed articles |
| Aggregate tickers | Every 15 min, 9AM-4PM | Update ticker sentiment |
| Market summary | Daily at 5PM IST | Generate daily summary |
| Cleanup | Weekly, Sunday 2AM | Remove old articles |
| Clear cache | Daily, 3AM | Clear expired cache |

## ğŸ”§ Configuration

### Celery Configuration
```python
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
CELERY_TASK_TIME_LIMIT = 300  # 5 minutes
CELERY_TIMEZONE = 'Asia/Kolkata'
```

### Cache Configuration
```python
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://localhost:6379/1',
        'TIMEOUT': 300,  # 5 minutes
    }
}
```

### REST Framework Configuration
```python
REST_FRAMEWORK = {
    'PAGE_SIZE': 20,
    'DEFAULT_THROTTLE_RATES': {
        'anon': '100/hour',
        'user': '1000/hour'
    }
}
```

## ğŸ§ª Testing

```bash
# Run all tests
python manage.py test

# Run specific test module
python manage.py test news_analyser.tests.test_sentiment

# Run with coverage
coverage run --source='.' manage.py test
coverage report
```

## ğŸ“ˆ Performance Metrics

- **Scraping**: 100+ articles/minute
- **Sentiment Analysis**: 60 articles/minute (Gemini API rate limit)
- **API Response**: <500ms (p95)
- **Cache Hit Rate**: >80% for ticker sentiment
- **Ticker Recognition**: >90% accuracy

## ğŸ”’ Security Features

- CSRF protection enabled
- Rate limiting on API endpoints (100 req/hour anonymous, 1000 req/hour authenticated)
- Environment variable-based secrets
- SQL injection prevention via Django ORM
- XSS protection via Django templates

## ğŸ“ Logging

Logs are stored in `/logs/`:
- `news_analyser.log` - General application logs
- `errors.log` - Error-level logs only

Log rotation: 10MB per file, 10 backup files

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is private and proprietary for Wall Street Club, BITS Pilani.

## ğŸ‘¥ Authors

- **Development Team** - Wall Street Club, BITS Pilani

## ğŸ™ Acknowledgments

- Google Gemini AI for sentiment analysis
- FinBERT team for finance-specific NLP model
- NSE India for ticker data
- Indian financial news sources for content

## ğŸ“ Support

For issues, feature requests, or questions, please contact the Wall Street Club development team.

---

**Built with â¤ï¸ for Wall Street Club, BITS Pilani**
